import os
import json
import requests
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv

load_dotenv()

# ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
IAS_API_BASE_URL = os.getenv("IAS_API_BASE_URL", "http://dmx-api.zj.sgcc.com.cn")
IAS_API_KEY = os.getenv("IAS_API_KEY", "33768df4e44e41d2a5a621065fa7d552")
IAS_MODEL = os.getenv("IAS_MODEL", "[2.0-æ¨¡å‹ä¸­å¿ƒ]Qwen3-32B-A100")


class LLMIasApi:
    """å›½ç½‘æ™ºèƒ½åˆ†æå¹³å°å¤§æ¨¡å‹æ¥å£å®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.base_url = IAS_API_BASE_URL
        self.api_key = IAS_API_KEY
        self.model = IAS_MODEL
        
    def _do_request(
        self, 
        endpoint: str, 
        data: Dict[str, Any], 
        content_type: str = "application/json;charset=utf-8"
    ) -> Dict[str, Any]:
        """
        æ‰§è¡ŒHTTPè¯·æ±‚çš„åº•å±‚æ–¹æ³•
        
        Args:
            endpoint: APIç«¯ç‚¹è·¯å¾„ï¼ˆå¦‚ /lmp-cloud-ias-server/api/llm/chat/completions/ï¼‰
            data: è¯·æ±‚ä½“æ•°æ®
            content_type: å†…å®¹ç±»å‹
            
        Returns:
            APIå“åº”çš„JSONæ•°æ®
        """
        # æ„é€ å®Œæ•´URL
        url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
        
        # æ„é€ è¯·æ±‚å¤´
        headers = {
            "Content-Type": content_type,
            "Authorization": self.api_key
        }
        
        # ========== è¯·æ±‚å‰æ‰“å° ==========
        print("=" * 80)
        print("ğŸš€ LLM API è¯·æ±‚å¼€å§‹")
        print("=" * 80)
        print(f"ğŸ“ URL: {url}")
        print(f"ğŸ”‘ Authorization: {self.api_key[:20]}...{self.api_key[-10:]}")  # éšè—ä¸­é—´éƒ¨åˆ†
        print(f"ğŸ“ Content-Type: {content_type}")
        print(f"ğŸ“¦ è¯·æ±‚æ•°æ®:")
        print(json.dumps(data, indent=2, ensure_ascii=False))
        print("=" * 80)
        
        try:
            # å‘é€POSTè¯·æ±‚
            response = requests.post(
                url,
                headers=headers,
                json=data,
                timeout=60
            )
            
            
            # è§£æJSONå“åº”
            result = response.json()
            
            # ========== è¯·æ±‚æˆåŠŸåæ‰“å° ==========
            print("=" * 80)
            print("âœ… LLM API è¯·æ±‚æˆåŠŸ")
            print("=" * 80)
            print(f"ğŸ“Š çŠ¶æ€ç : {response.status_code}")
            print(f"â±ï¸  å“åº”æ—¶é—´: {response.elapsed.total_seconds():.2f}ç§’")
            print(f"ğŸ“¥ å“åº”æ•°æ®:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
            print("=" * 80)
            
            return result
            
        except requests.exceptions.Timeout:
            # ========== è¶…æ—¶é”™è¯¯æ‰“å° ==========
            print("=" * 80)
            print("â° LLM API è¯·æ±‚è¶…æ—¶")
            print("=" * 80)
            print(f"âŒ é”™è¯¯ç±»å‹: Timeout")
            print(f"â±ï¸  è¶…æ—¶æ—¶é—´: 60ç§’")
            print("=" * 80)
            
            return {
                "error": {
                    "type": "timeout_error",
                    "message": "è¯·æ±‚è¶…æ—¶"
                }
            }
        except requests.exceptions.HTTPError as e:
            # ========== HTTPé”™è¯¯æ‰“å° ==========
            print("=" * 80)
            print("âŒ LLM API HTTP é”™è¯¯")
            print("=" * 80)
            print(f"ğŸ“Š çŠ¶æ€ç : {e.response.status_code}")
            print(f"ğŸ“„ é”™è¯¯è¯¦æƒ…:")
            print(e.response.text)
            print("=" * 80)
            
            return {
                "error": {
                    "type": "http_error",
                    "message": f"HTTPé”™è¯¯: {e.response.status_code}",
                    "details": e.response.text
                }
            }
        except requests.exceptions.RequestException as e:
            # ========== è¯·æ±‚å¼‚å¸¸æ‰“å° ==========
            print("=" * 80)
            print("âŒ LLM API è¯·æ±‚å¼‚å¸¸")
            print("=" * 80)
            print(f"âš ï¸  å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“„ å¼‚å¸¸ä¿¡æ¯: {str(e)}")
            print("=" * 80)
            
            return {
                "error": {
                    "type": "request_error",
                    "message": f"è¯·æ±‚å¼‚å¸¸: {str(e)}"
                }
            }
        except json.JSONDecodeError:
            # ========== JSONè§£æé”™è¯¯æ‰“å° ==========
            raw_text = response.text if 'response' in locals() else None
            print("=" * 80)
            print("âŒ LLM API JSON è§£æå¤±è´¥")
            print("=" * 80)
            print(f"ğŸ“„ åŸå§‹å“åº”å†…å®¹:")
            print(raw_text[:500] if raw_text else "æ— å“åº”å†…å®¹")  # åªæ‰“å°å‰500å­—ç¬¦
            print("=" * 80)
            
            return {
                "error": {
                    "type": "parse_error",
                    "message": "å“åº”JSONè§£æå¤±è´¥",
                    "raw_response": raw_text
                }
            }
    
    def chat_completions(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        stream: bool = False,
        temperature: float = 0.95,
        top_p: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        
        # ========== FAKE æ•°æ®åŒºåŸŸ - æµ‹è¯•æ—¶æ‰‹åŠ¨åˆ‡æ¢æ³¨é‡Š ==========
        
        # # FAKE 1: æˆåŠŸå“åº” - æ ‡å‡†JSONæ ¼å¼
        # return {
        #     "id": "chatcmpl-test-12345",
        #     "object": "chat.completion",
        #     "created": 1699999999,
        #     "choices": [
        #         {
        #             "index": 0,
        #             "message": {
        #                 "role": "assistant",
        #                 "content": '{"score": 85, "reason": "fakefakeé¡¹ç›®åç§°é«˜åº¦ç›¸ä¼¼---1111"}'
        #             },
        #             "finish_reason": "stop"
        #         }
        #     ],
        #     "usage": {
        #         "prompt_tokens": 50,
        #         "completion_tokens": 30,
        #         "total_tokens": 80
        #     }
        # }
        
        # # FAKE 2: æˆåŠŸå“åº” - å¸¦Markdownä»£ç å—çš„JSON
        # return {
        #     "id": "chatcmpl-test-12346",
        #     "object": "chat.completion",
        #     "created": 1699999999,
        #     "choices": [
        #         {
        #             "index": 0,
        #             "message": {
        #                 "role": "assistant",
        #                 "content": '```json\n{"score": 92, "reason": "å†…å®¹å‡ ä¹å®Œå…¨ä¸€è‡´"}\n```'
        #             },
        #             "finish_reason": "stop"
        #         }
        #     ],
        #     "usage": {
        #         "prompt_tokens": 50,
        #         "completion_tokens": 30,
        #         "total_tokens": 80
        #     }
        # }
        
        # # FAKE 3: æˆåŠŸå“åº” - ä½ç›¸ä¼¼åº¦
        # return {
        #     "id": "chatcmpl-test-12347",
        #     "object": "chat.completion",
        #     "created": 1699999999,
        #     "choices": [
        #         {
        #             "index": 0,
        #             "message": {
        #                 "role": "assistant",
        #                 "content": '{"score": 15, "reason": "é¡¹ç›®ç±»å‹å®Œå…¨ä¸åŒ"}'
        #             },
        #             "finish_reason": "stop"
        #         }
        #     ],
        #     "usage": {
        #         "prompt_tokens": 50,
        #         "completion_tokens": 30,
        #         "total_tokens": 80
        #     }
        # }
        
        # # FAKE 4: é”™è¯¯å“åº” - è¶…æ—¶é”™è¯¯
        # return {
        #     "error": {
        #         "type": "timeout_error",
        #         "message": "è¯·æ±‚è¶…æ—¶"
        #     }
        # }
        
        # # FAKE 5: é”™è¯¯å“åº” - HTTPé”™è¯¯
        # return {
        #     "error": {
        #         "type": "http_error",
        #         "message": "HTTPé”™è¯¯: 500",
        #         "details": "Internal Server Error"
        #     }
        # }
        
        # # FAKE 6: é”™è¯¯å“åº” - æ ¼å¼é”™è¯¯ï¼ˆä¸ç¬¦åˆJSONè§„èŒƒï¼‰
        # return {
        #     "id": "chatcmpl-test-12348",
        #     "object": "chat.completion",
        #     "created": 1699999999,
        #     "choices": [
        #         {
        #             "index": 0,
        #             "message": {
        #                 "role": "assistant",
        #                 "content": "è¿™æ˜¯ä¸€ä¸ªæ— æ•ˆçš„JSONæ ¼å¼å“åº”"
        #             },
        #             "finish_reason": "stop"
        #         }
        #     ],
        #     "usage": {
        #         "prompt_tokens": 50,
        #         "completion_tokens": 30,
        #         "total_tokens": 80
        #     }
        # }
        
        # # FAKE 7: é”™è¯¯å“åº” - ç©ºchoices
        # return {
        #     "id": "chatcmpl-test-12349",
        #     "object": "chat.completion",
        #     "created": 1699999999,
        #     "choices": [],
        #     "usage": {
        #         "prompt_tokens": 50,
        #         "completion_tokens": 0,
        #         "total_tokens": 50
        #     }
        # }


        """
        è¯­ä¹‰å¤§æ¨¡å‹å¯¹è¯æ¥å£
        
        Args:
            messages: å¯¹è¯å†å²ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}]
            model: æ¨¡å‹IDï¼Œé»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„æ¨¡å‹
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            temperature: éšæœºæ€§æ§åˆ¶ï¼ŒèŒƒå›´(0, 1.0]ï¼Œé»˜è®¤0.95
            top_p: å¤šæ ·æ€§æ§åˆ¶ï¼ŒèŒƒå›´[0, 1.0]ï¼Œé»˜è®¤0.7
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            **kwargs: å…¶ä»–å¯é€‰å‚æ•°ï¼ˆpresence_penalty, tools, tool_choiceç­‰ï¼‰
            
        Returns:
            APIå“åº”æ•°æ®
            
        ç¤ºä¾‹:
            >>> api = LLMIasApi()
            >>> result = api.chat_completions([
            ...     {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹å›½ç½‘è¡¢å·ä¾›ç”µå…¬å¸"}
            ... ])
        """
        # æ„é€ è¯·æ±‚æ•°æ®
        request_data = {
            "model": model or self.model,
            "messages": messages,
            "stream": stream,
            "temperature": temperature,
            "top_p": top_p
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if max_tokens is not None:
            request_data["max_tokens"] = max_tokens
            
        # æ·»åŠ å…¶ä»–å¯é€‰å‚æ•°
        for key, value in kwargs.items():
            if key in ["presence_penalty", "tools", "tool_choice", "parallel_tool_calls"]:
                request_data[key] = value
        
        # å‘èµ·è¯·æ±‚
        endpoint = "/lmp-cloud-ias-server/api/llm/chat/completions/"
        
        return self._do_request(endpoint, request_data)
    
    def chat_completions_v2(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        è¯­ä¹‰å¤§æ¨¡å‹å¯¹è¯æ¥å£ V2 ç‰ˆæœ¬
        
        V2ç‰ˆæœ¬ä¸åŸæ¥å£çš„åŒºåˆ«ï¼šæµå¼è¾“å‡ºæ—¶æ²¡æœ‰event:dataäº‹ä»¶ç±»å‹
        
        Args:
            messages: å¯¹è¯å†å²
            model: æ¨¡å‹ID
            **kwargs: å…¶ä»–å‚æ•°åŒ chat_completions
            
        Returns:
            APIå“åº”æ•°æ®
        """
        # ä½¿ç”¨ V2 ç«¯ç‚¹
        endpoint = "/lmp-cloud-ias-server/api/llm/chat/completions/V2"
        
        request_data = {
            "model": model or self.model,
            "messages": messages,
            **kwargs
        }
        
        return self._do_request(endpoint, request_data)

